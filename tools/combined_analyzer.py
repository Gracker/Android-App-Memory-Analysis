#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
HPROF + smaps è”åˆå†…å­˜åˆ†æå·¥å…·

å°† Java å †åˆ†æ (HPROF) ä¸ Native å†…å­˜åˆ†æ (smaps) ç»“åˆï¼Œæä¾›å®Œæ•´çš„å†…å­˜è§†å›¾ã€‚

åŠŸèƒ½:
- å…³è” Java å †å’Œ Native å†…å­˜
- è¯†åˆ« Native å†…å­˜æ³„æ¼ï¼ˆHPROF ä¸­æœ‰ Bitmap ä½† smaps ä¸­ Graphics å†…å­˜å¼‚å¸¸ï¼‰
- åˆ†æ Java/Native å†…å­˜æ¯”ä¾‹
- æ£€æµ‹å†…å­˜ç¢ç‰‡åŒ–
- æä¾›ç»¼åˆä¼˜åŒ–å»ºè®®
"""

import argparse
import os
import sys
from datetime import datetime
from collections import defaultdict

# Import parsers
from hprof_parser import HprofParser

# smaps parser uses global variables, we need to import them
import smaps_parser


class CombinedAnalyzer:
    """HPROF + smaps è”åˆåˆ†æå™¨"""

    def __init__(self, hprof_file, smaps_file):
        self.hprof_file = hprof_file
        self.smaps_file = smaps_file
        self.hprof_data = None
        self.smaps_data = None

    def parse_hprof(self):
        """è§£æ HPROF æ–‡ä»¶"""
        print(f"\n{'='*60}")
        print(f"æ­£åœ¨è§£æ HPROF æ–‡ä»¶: {self.hprof_file}")
        print('='*60)

        parser = HprofParser(self.hprof_file)
        parser.parse(simple_mode=True, deep_analysis=True)

        self.hprof_data = {
            'total_instances': parser.total_instances,
            'total_instance_size': parser.total_instance_size,
            'total_arrays': parser.total_arrays,
            'total_array_size': parser.total_array_size,
            'total_java_heap': (parser.total_instance_size + parser.total_array_size) / 1024 / 1024,
            'bitmap_info': parser.bitmap_info,
            'class_stats': parser.class_stats,
            'package_stats': parser.package_stats,
            'string_stats': parser.string_stats,
            'primitive_stats': parser.primitive_stats,
            'leak_suspects': parser.leak_suspects,
            'app_package': parser.app_package,
            'large_byte_arrays': getattr(parser, 'large_byte_arrays', []),
            'lru_cache_analysis': getattr(parser, 'lru_cache_analysis', []),
            'empty_collections': getattr(parser, 'empty_collections', []),
            'large_collections': getattr(parser, 'large_collections', []),
        }

        return self.hprof_data

    def parse_smaps(self):
        """è§£æ smaps æ–‡ä»¶"""
        print(f"\n{'='*60}")
        print(f"æ­£åœ¨è§£æ smaps æ–‡ä»¶: {self.smaps_file}")
        print('='*60)

        # Reset global counters
        smaps_parser.pss_count = [0] * smaps_parser.type_length
        smaps_parser.pssSum_count = [0] * smaps_parser.type_length
        smaps_parser.swapPss_count = [0] * smaps_parser.type_length

        # Reset type lists
        for i in range(smaps_parser.type_length):
            smaps_parser.pss_type_list[i] = {}
            smaps_parser.swap_type_list[i] = {}

        # Parse smaps
        smaps_parser.parse_smaps(self.smaps_file)

        # Extract data
        self.smaps_data = {
            'total_pss_kb': sum(smaps_parser.pssSum_count),
            'total_swap_kb': sum(smaps_parser.swapPss_count),

            # Major categories (KB)
            'dalvik_heap_kb': smaps_parser.pss_count[smaps_parser.HEAP_DALVIK],
            'native_heap_kb': smaps_parser.pss_count[smaps_parser.HEAP_NATIVE],
            'dalvik_other_kb': smaps_parser.pss_count[smaps_parser.HEAP_DALVIK_OTHER],
            'stack_kb': smaps_parser.pss_count[smaps_parser.HEAP_STACK],
            'so_kb': smaps_parser.pss_count[smaps_parser.HEAP_SO],
            'dex_kb': smaps_parser.pss_count[smaps_parser.HEAP_DEX],
            'oat_kb': smaps_parser.pss_count[smaps_parser.HEAP_OAT],
            'art_kb': smaps_parser.pss_count[smaps_parser.HEAP_ART],
            'graphics_kb': smaps_parser.pss_count[smaps_parser.HEAP_GRAPHICS],
            'gl_kb': smaps_parser.pss_count[smaps_parser.HEAP_GL],
            'apk_kb': smaps_parser.pss_count[smaps_parser.HEAP_APK],
            'ttf_kb': smaps_parser.pss_count[smaps_parser.HEAP_TTF],
            'ashmem_kb': smaps_parser.pss_count[smaps_parser.HEAP_ASHMEM],
            'unknown_kb': smaps_parser.pss_count[smaps_parser.HEAP_UNKNOWN],

            # Dalvik subdivisions
            'dalvik_normal_kb': smaps_parser.pss_count[smaps_parser.HEAP_DALVIK_NORMAL],
            'dalvik_large_kb': smaps_parser.pss_count[smaps_parser.HEAP_DALVIK_LARGE],
            'dalvik_zygote_kb': smaps_parser.pss_count[smaps_parser.HEAP_DALVIK_ZYGOTE],
            'dalvik_non_moving_kb': smaps_parser.pss_count[smaps_parser.HEAP_DALVIK_NON_MOVING],

            # Raw data for detailed analysis
            'pss_count': list(smaps_parser.pss_count),
            'pss_type_list': [dict(d) for d in smaps_parser.pss_type_list],
        }

        return self.smaps_data

    def analyze(self):
        """æ‰§è¡Œè”åˆåˆ†æ"""
        if not self.hprof_data:
            self.parse_hprof()
        if not self.smaps_data:
            self.parse_smaps()

        results = {
            'memory_overview': self._analyze_memory_overview(),
            'java_native_correlation': self._analyze_java_native_correlation(),
            'bitmap_graphics_analysis': self._analyze_bitmap_graphics(),
            'memory_distribution': self._analyze_memory_distribution(),
            'anomalies': self._detect_anomalies(),
            'recommendations': self._generate_recommendations(),
        }

        return results

    def _analyze_memory_overview(self):
        """å†…å­˜æ¦‚è§ˆåˆ†æ"""
        java_heap_mb = self.hprof_data['total_java_heap']
        total_pss_mb = self.smaps_data['total_pss_kb'] / 1024
        native_heap_mb = self.smaps_data['native_heap_kb'] / 1024
        dalvik_heap_mb = self.smaps_data['dalvik_heap_kb'] / 1024

        return {
            'java_heap_mb': java_heap_mb,
            'dalvik_heap_smaps_mb': dalvik_heap_mb,
            'native_heap_mb': native_heap_mb,
            'total_pss_mb': total_pss_mb,
            'java_native_ratio': java_heap_mb / native_heap_mb if native_heap_mb > 0 else 0,
            'heap_diff_mb': abs(java_heap_mb - dalvik_heap_mb),
            'heap_diff_pct': abs(java_heap_mb - dalvik_heap_mb) / dalvik_heap_mb * 100 if dalvik_heap_mb > 0 else 0,
        }

    def _analyze_java_native_correlation(self):
        """Java/Native å†…å­˜å…³è”åˆ†æ"""
        # Estimate native memory from Java objects
        bitmap_native_estimate = 0
        if self.hprof_data['bitmap_info']:
            for bitmap_id, info in self.hprof_data['bitmap_info'].items():
                # Native pixel data
                bitmap_native_estimate += info.get('estimated_size', 0)

        # byte[] that might be native-backed
        byte_array_size = self.hprof_data['primitive_stats'].get('byte[]', {}).get('size', 0) / 1024

        # Actual native heap from smaps
        native_heap_kb = self.smaps_data['native_heap_kb']

        return {
            'estimated_bitmap_native_kb': bitmap_native_estimate / 1024,
            'byte_array_size_kb': byte_array_size,
            'actual_native_heap_kb': native_heap_kb,
            'unaccounted_native_kb': native_heap_kb - (bitmap_native_estimate / 1024) - byte_array_size,
        }

    def _analyze_bitmap_graphics(self):
        """Bitmap ä¸ Graphics å†…å­˜å…³è”åˆ†æ"""
        # HPROF: Bitmap statistics
        bitmap_count = len(self.hprof_data['bitmap_info'])
        bitmap_total_size = sum(info.get('estimated_size', 0)
                               for info in self.hprof_data['bitmap_info'].values())

        # smaps: Graphics memory
        graphics_kb = self.smaps_data['graphics_kb']
        gl_kb = self.smaps_data['gl_kb']

        return {
            'bitmap_count': bitmap_count,
            'bitmap_java_size_mb': bitmap_total_size / 1024 / 1024,
            'graphics_memory_mb': graphics_kb / 1024,
            'gl_memory_mb': gl_kb / 1024,
            'total_graphics_mb': (graphics_kb + gl_kb) / 1024,
            'bitmap_per_graphics_ratio': (bitmap_total_size / 1024) / (graphics_kb + gl_kb) if (graphics_kb + gl_kb) > 0 else 0,
        }

    def _analyze_memory_distribution(self):
        """å†…å­˜åˆ†å¸ƒåˆ†æ"""
        total_pss = self.smaps_data['total_pss_kb']
        if total_pss == 0:
            return {}

        distribution = {
            'dalvik_heap_pct': self.smaps_data['dalvik_heap_kb'] / total_pss * 100,
            'native_heap_pct': self.smaps_data['native_heap_kb'] / total_pss * 100,
            'code_pct': (self.smaps_data['so_kb'] + self.smaps_data['dex_kb'] +
                        self.smaps_data['oat_kb'] + self.smaps_data['art_kb']) / total_pss * 100,
            'graphics_pct': (self.smaps_data['graphics_kb'] + self.smaps_data['gl_kb']) / total_pss * 100,
            'stack_pct': self.smaps_data['stack_kb'] / total_pss * 100,
            'other_pct': self.smaps_data['unknown_kb'] / total_pss * 100,
        }

        # Detailed code breakdown
        distribution['code_breakdown'] = {
            'so_mb': self.smaps_data['so_kb'] / 1024,
            'dex_mb': self.smaps_data['dex_kb'] / 1024,
            'oat_mb': self.smaps_data['oat_kb'] / 1024,
            'art_mb': self.smaps_data['art_kb'] / 1024,
        }

        return distribution

    def _detect_anomalies(self):
        """æ£€æµ‹å¼‚å¸¸"""
        anomalies = []

        overview = self._analyze_memory_overview()
        correlation = self._analyze_java_native_correlation()
        bitmap_analysis = self._analyze_bitmap_graphics()

        # 1. Java heap vs Dalvik heap mismatch
        if overview['heap_diff_pct'] > 20:
            anomalies.append({
                'type': 'HEAP_MISMATCH',
                'severity': 'MEDIUM',
                'description': f"Java å † ({overview['java_heap_mb']:.1f} MB) ä¸ smaps Dalvik å † "
                              f"({overview['dalvik_heap_smaps_mb']:.1f} MB) å·®å¼‚è¶…è¿‡ 20%",
                'suggestion': 'å¯èƒ½å­˜åœ¨å †å¤–å†…å­˜ä½¿ç”¨æˆ– GC åæœªé‡Šæ”¾çš„åŒºåŸŸ'
            })

        # 2. Large unaccounted native memory
        unaccounted = correlation['unaccounted_native_kb']
        if unaccounted > 10 * 1024:  # > 10MB
            anomalies.append({
                'type': 'UNACCOUNTED_NATIVE',
                'severity': 'HIGH',
                'description': f"å‘ç° {unaccounted/1024:.1f} MB æ— æ³•è¿½è¸ªçš„ Native å†…å­˜",
                'suggestion': 'æ£€æŸ¥ C/C++ ä»£ç ä¸­çš„å†…å­˜åˆ†é…ï¼Œå¯èƒ½å­˜åœ¨ Native æ³„æ¼'
            })

        # 3. Graphics memory anomaly
        if bitmap_analysis['bitmap_count'] == 0 and bitmap_analysis['total_graphics_mb'] > 50:
            anomalies.append({
                'type': 'GRAPHICS_WITHOUT_BITMAP',
                'severity': 'MEDIUM',
                'description': f"æ—  Bitmap å¯¹è±¡ä½† Graphics å†…å­˜é«˜è¾¾ {bitmap_analysis['total_graphics_mb']:.1f} MB",
                'suggestion': 'å¯èƒ½ä½¿ç”¨äº† SurfaceViewã€TextureView æˆ– OpenGL ç›´æ¥æ¸²æŸ“'
            })

        # 4. High native/java ratio
        if overview['java_native_ratio'] < 0.5:
            anomalies.append({
                'type': 'HIGH_NATIVE_RATIO',
                'severity': 'INFO',
                'description': f"Native å†…å­˜ ({overview['native_heap_mb']:.1f} MB) è¿œè¶… Java å † "
                              f"({overview['java_heap_mb']:.1f} MB)",
                'suggestion': 'åº”ç”¨å¯èƒ½å¤§é‡ä½¿ç”¨ NDK æˆ–ç¬¬ä¸‰æ–¹ Native åº“'
            })

        # 5. Large stack memory (too many threads)
        stack_mb = self.smaps_data['stack_kb'] / 1024
        if stack_mb > 20:  # > 20MB
            estimated_threads = int(stack_mb / 1)  # ~1MB per thread
            anomalies.append({
                'type': 'HIGH_STACK_MEMORY',
                'severity': 'MEDIUM',
                'description': f"æ ˆå†…å­˜ {stack_mb:.1f} MBï¼Œä¼°è®¡çº¦ {estimated_threads} ä¸ªçº¿ç¨‹",
                'suggestion': 'æ£€æŸ¥æ˜¯å¦åˆ›å»ºäº†è¿‡å¤šçº¿ç¨‹ï¼Œè€ƒè™‘ä½¿ç”¨çº¿ç¨‹æ± '
            })

        # 6. High swap usage
        swap_mb = self.smaps_data['total_swap_kb'] / 1024
        if swap_mb > 50:
            anomalies.append({
                'type': 'HIGH_SWAP_USAGE',
                'severity': 'HIGH',
                'description': f"Swap ä½¿ç”¨é‡ {swap_mb:.1f} MB",
                'suggestion': 'åº”ç”¨å†…å­˜å‹åŠ›å¤§ï¼Œéƒ¨åˆ†å†…å­˜è¢«äº¤æ¢åˆ°ç£ç›˜ï¼Œä¼šå½±å“æ€§èƒ½'
            })

        return anomalies

    def _generate_recommendations(self):
        """ç”Ÿæˆç»¼åˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        anomalies = self._detect_anomalies()

        # Based on anomalies
        for anomaly in anomalies:
            if anomaly['severity'] in ['HIGH', 'MEDIUM']:
                recommendations.append({
                    'priority': anomaly['severity'],
                    'area': anomaly['type'],
                    'suggestion': anomaly['suggestion']
                })

        # Based on HPROF analysis
        if self.hprof_data['leak_suspects']:
            recommendations.append({
                'priority': 'HIGH',
                'area': 'MEMORY_LEAK',
                'suggestion': f"æ£€æµ‹åˆ° {len(self.hprof_data['leak_suspects'])} ä¸ªæ³„æ¼å«Œç–‘ï¼Œä¼˜å…ˆå¤„ç†"
            })

        # Based on distribution
        distribution = self._analyze_memory_distribution()
        if distribution.get('code_pct', 0) > 40:
            recommendations.append({
                'priority': 'MEDIUM',
                'area': 'CODE_SIZE',
                'suggestion': 'ä»£ç å ç”¨å†…å­˜è¿‡å¤§ï¼Œè€ƒè™‘ä½¿ç”¨åŠ¨æ€åŠ è½½æˆ– App Bundle'
            })

        # Empty collections
        if self.hprof_data['empty_collections']:
            total_empty = sum(count for _, count in self.hprof_data['empty_collections'])
            if total_empty > 500:
                recommendations.append({
                    'priority': 'LOW',
                    'area': 'EMPTY_COLLECTIONS',
                    'suggestion': f'{total_empty} ä¸ªç©ºé›†åˆï¼Œä½¿ç”¨ Collections.emptyXxx() æˆ–å»¶è¿Ÿåˆå§‹åŒ–'
                })

        return recommendations

    def print_report(self):
        """æ‰“å°åˆ†ææŠ¥å‘Š"""
        results = self.analyze()

        print("\n" + "="*80)
        print("=== HPROF + smaps è”åˆå†…å­˜åˆ†ææŠ¥å‘Š ===")
        print("="*80)

        # Overview
        overview = results['memory_overview']
        print("\n--- å†…å­˜æ¦‚è§ˆ ---")
        print(f"{'æŒ‡æ ‡':<30} {'å€¼':>15}")
        print("-" * 50)
        print(f"{'Java å † (HPROF)':<30} {overview['java_heap_mb']:>12.2f} MB")
        print(f"{'Dalvik å † (smaps)':<30} {overview['dalvik_heap_smaps_mb']:>12.2f} MB")
        print(f"{'Native å † (smaps)':<30} {overview['native_heap_mb']:>12.2f} MB")
        print(f"{'æ€» PSS (smaps)':<30} {overview['total_pss_mb']:>12.2f} MB")
        print(f"{'Java/Native æ¯”ä¾‹':<30} {overview['java_native_ratio']:>12.2f}")

        # Distribution
        dist = results['memory_distribution']
        if dist:
            print("\n--- å†…å­˜åˆ†å¸ƒ (smaps) ---")
            print(f"{'åŒºåŸŸ':<20} {'å æ¯”':>10} {'è¯´æ˜':<30}")
            print("-" * 60)
            print(f"{'Dalvik å †':<20} {dist['dalvik_heap_pct']:>8.1f}% {'Java å¯¹è±¡':<30}")
            print(f"{'Native å †':<20} {dist['native_heap_pct']:>8.1f}% {'C/C++ åˆ†é…çš„å†…å­˜':<30}")
            print(f"{'ä»£ç ':<20} {dist['code_pct']:>8.1f}% {'.so/.dex/.oat/.art':<30}")
            print(f"{'å›¾å½¢':<20} {dist['graphics_pct']:>8.1f}% {'GPU ç›¸å…³å†…å­˜':<30}")
            print(f"{'æ ˆ':<20} {dist['stack_pct']:>8.1f}% {'çº¿ç¨‹æ ˆ':<30}")

        # Bitmap/Graphics correlation
        bitmap = results['bitmap_graphics_analysis']
        if bitmap['bitmap_count'] > 0 or bitmap['total_graphics_mb'] > 0:
            print("\n--- Bitmap/Graphics å…³è”åˆ†æ ---")
            print(f"HPROF Bitmap å¯¹è±¡æ•°: {bitmap['bitmap_count']}")
            print(f"HPROF Bitmap ä¼°ç®—å¤§å°: {bitmap['bitmap_java_size_mb']:.2f} MB")
            print(f"smaps Graphics å†…å­˜: {bitmap['graphics_memory_mb']:.2f} MB")
            print(f"smaps GL å†…å­˜: {bitmap['gl_memory_mb']:.2f} MB")

        # Java/Native correlation
        corr = results['java_native_correlation']
        print("\n--- Java/Native å†…å­˜å…³è” ---")
        print(f"ä¼°ç®— Bitmap Native å†…å­˜: {corr['estimated_bitmap_native_kb']/1024:.2f} MB")
        print(f"byte[] æ•°ç»„å¤§å°: {corr['byte_array_size_kb']/1024:.2f} MB")
        print(f"å®é™… Native å †: {corr['actual_native_heap_kb']/1024:.2f} MB")
        if corr['unaccounted_native_kb'] > 0:
            print(f"âš ï¸  æœªè¿½è¸ª Native å†…å­˜: {corr['unaccounted_native_kb']/1024:.2f} MB")

        # Anomalies
        anomalies = results['anomalies']
        if anomalies:
            print("\n--- æ£€æµ‹åˆ°çš„å¼‚å¸¸ ---")
            severity_icons = {'HIGH': 'ğŸ”´', 'MEDIUM': 'ğŸŸ¡', 'INFO': 'ğŸ”µ', 'LOW': 'ğŸŸ¢'}
            for i, anomaly in enumerate(anomalies, 1):
                icon = severity_icons.get(anomaly['severity'], 'âšª')
                print(f"\n{icon} [{i}] {anomaly['type']}")
                print(f"   {anomaly['description']}")
                print(f"   ğŸ’¡ {anomaly['suggestion']}")

        # Recommendations
        recommendations = results['recommendations']
        if recommendations:
            print("\n--- ä¼˜åŒ–å»ºè®® ---")
            for i, rec in enumerate(recommendations, 1):
                priority = rec['priority']
                icon = 'ğŸ”´' if priority == 'HIGH' else ('ğŸŸ¡' if priority == 'MEDIUM' else 'ğŸŸ¢')
                print(f"{icon} [{rec['area']}] {rec['suggestion']}")

        print("\n" + "="*80)

    def export_markdown(self, output_file):
        """å¯¼å‡º Markdown æ ¼å¼æŠ¥å‘Š"""
        results = self.analyze()

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("# HPROF + smaps è”åˆå†…å­˜åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  \n")
            f.write(f"**HPROF æ–‡ä»¶**: `{os.path.basename(self.hprof_file)}`  \n")
            f.write(f"**smaps æ–‡ä»¶**: `{os.path.basename(self.smaps_file)}`\n\n")

            if self.hprof_data.get('app_package'):
                f.write(f"**App åŒ…å**: `{self.hprof_data['app_package']}`\n\n")

            f.write("---\n\n")

            # Overview
            overview = results['memory_overview']
            f.write("## å†…å­˜æ¦‚è§ˆ\n\n")
            f.write("| æŒ‡æ ‡ | å€¼ | è¯´æ˜ |\n")
            f.write("|------|-----|------|\n")
            f.write(f"| Java å † (HPROF) | **{overview['java_heap_mb']:.2f} MB** | HPROF ä¸­çš„å¯¹è±¡æ€»å¤§å° |\n")
            f.write(f"| Dalvik å † (smaps) | {overview['dalvik_heap_smaps_mb']:.2f} MB | smaps ä¸­çš„ Dalvik å † |\n")
            f.write(f"| Native å † (smaps) | {overview['native_heap_mb']:.2f} MB | C/C++ åˆ†é…çš„å†…å­˜ |\n")
            f.write(f"| æ€» PSS | **{overview['total_pss_mb']:.2f} MB** | è¿›ç¨‹å®é™…ç‰©ç†å†…å­˜å ç”¨ |\n")
            f.write(f"| Java/Native æ¯”ä¾‹ | {overview['java_native_ratio']:.2f} | >1 è¡¨ç¤º Java ä¸ºä¸» |\n\n")

            # Distribution pie chart (text representation)
            dist = results['memory_distribution']
            if dist:
                f.write("## å†…å­˜åˆ†å¸ƒ\n\n")
                f.write("```\n")
                f.write(f"Dalvik å †: {'â–ˆ' * int(dist['dalvik_heap_pct']/5)} {dist['dalvik_heap_pct']:.1f}%\n")
                f.write(f"Native å †: {'â–ˆ' * int(dist['native_heap_pct']/5)} {dist['native_heap_pct']:.1f}%\n")
                f.write(f"ä»£ç :      {'â–ˆ' * int(dist['code_pct']/5)} {dist['code_pct']:.1f}%\n")
                f.write(f"å›¾å½¢:      {'â–ˆ' * int(dist['graphics_pct']/5)} {dist['graphics_pct']:.1f}%\n")
                f.write(f"æ ˆ:        {'â–ˆ' * int(dist['stack_pct']/5)} {dist['stack_pct']:.1f}%\n")
                f.write("```\n\n")

            # Anomalies
            anomalies = results['anomalies']
            if anomalies:
                f.write("## æ£€æµ‹åˆ°çš„å¼‚å¸¸\n\n")
                for anomaly in anomalies:
                    severity = anomaly['severity']
                    icon = 'ğŸ”´' if severity == 'HIGH' else ('ğŸŸ¡' if severity == 'MEDIUM' else 'ğŸ”µ')
                    f.write(f"### {icon} {anomaly['type']}\n\n")
                    f.write(f"> {anomaly['description']}\n\n")
                    f.write(f"**å»ºè®®**: {anomaly['suggestion']}\n\n")

            # Recommendations
            recommendations = results['recommendations']
            if recommendations:
                f.write("## ä¼˜åŒ–å»ºè®®\n\n")
                for rec in recommendations:
                    priority = rec['priority']
                    icon = 'ğŸ”´' if priority == 'HIGH' else ('ğŸŸ¡' if priority == 'MEDIUM' else 'ğŸŸ¢')
                    f.write(f"- {icon} **[{rec['area']}]** {rec['suggestion']}\n")
                f.write("\n")

            f.write("---\n\n")
            f.write("*æŠ¥å‘Šç”± Android-App-Memory-Analysis è”åˆåˆ†æå·¥å…·ç”Ÿæˆ*\n")

        print(f"\næŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {output_file}")


def main():
    parser = argparse.ArgumentParser(
        description="HPROF + smaps è”åˆå†…å­˜åˆ†æå·¥å…·",
        epilog="ç¤ºä¾‹:\n"
               "  python3 combined_analyzer.py -H dump.hprof -S smaps.txt\n"
               "  python3 combined_analyzer.py -H dump.hprof -S smaps.txt --markdown",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument('-H', '--hprof', required=True, help="HPROF æ–‡ä»¶è·¯å¾„")
    parser.add_argument('-S', '--smaps', required=True, help="smaps æ–‡ä»¶è·¯å¾„")
    parser.add_argument('-o', '--output', help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument('--markdown', action='store_true', help="è¾“å‡º Markdown æ ¼å¼")

    args = parser.parse_args()

    if not os.path.exists(args.hprof):
        print(f"é”™è¯¯: HPROF æ–‡ä»¶ä¸å­˜åœ¨: {args.hprof}")
        sys.exit(1)

    if not os.path.exists(args.smaps):
        print(f"é”™è¯¯: smaps æ–‡ä»¶ä¸å­˜åœ¨: {args.smaps}")
        sys.exit(1)

    analyzer = CombinedAnalyzer(args.hprof, args.smaps)
    analyzer.print_report()

    if args.markdown or args.output:
        output_file = args.output or "combined_analysis.md"
        if not output_file.endswith('.md'):
            output_file += '.md'
        analyzer.export_markdown(output_file)


if __name__ == '__main__':
    main()
