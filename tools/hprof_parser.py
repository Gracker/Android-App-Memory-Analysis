#!/usr/bin/env python
# -*- coding:utf-8 -*-
# Android HPROF Memory Analysis Tool
# @Author: Android Memory Analysis Tool

import argparse
import struct
import os
import sys
from collections import defaultdict, Counter
from datetime import datetime

class HprofRecord:
    """HPROFè®°å½•ç±»å‹"""
    UTF8 = 0x01
    LOAD_CLASS = 0x02
    UNLOAD_CLASS = 0x03
    STACK_FRAME = 0x04
    STACK_TRACE = 0x05
    ALLOC_SITES = 0x06
    HEAP_SUMMARY = 0x07
    START_THREAD = 0x0a
    END_THREAD = 0x0b
    HEAP_DUMP = 0x0c
    HEAP_DUMP_SEGMENT = 0x1c
    HEAP_DUMP_END = 0x2c
    CPU_SAMPLES = 0x0d
    CONTROL_SETTINGS = 0x0e

class HprofHeapDump:
    """å †dumpå­è®°å½•ç±»å‹"""
    ROOT_UNKNOWN = 0xff
    ROOT_JNI_GLOBAL = 0x01
    ROOT_JNI_LOCAL = 0x02
    ROOT_JAVA_FRAME = 0x03
    ROOT_NATIVE_STACK = 0x04
    ROOT_STICKY_CLASS = 0x05
    ROOT_THREAD_BLOCK = 0x06
    ROOT_MONITOR_USED = 0x07
    ROOT_THREAD_OBJECT = 0x08
    CLASS_DUMP = 0x20
    INSTANCE_DUMP = 0x21
    OBJECT_ARRAY_DUMP = 0x22
    PRIMITIVE_ARRAY_DUMP = 0x23

class HprofParser:
    def __init__(self, filename):
        self.filename = filename
        self.file = None
        self.header = {}
        self.strings = {}  # id -> string
        self.classes = {}  # class_id -> class_info
        self.instances = {}  # object_id -> instance_info
        self.arrays = {}  # array_id -> array_info
        self.roots = []
        self.total_instances = 0
        self.total_instance_size = 0
        self.total_arrays = 0
        self.total_array_size = 0
        
        # åˆ†ç±»ç»Ÿè®¡
        self.class_stats = defaultdict(lambda: {'count': 0, 'size': 0})
        self.string_stats = {'count': 0, 'size': 0}
        self.primitive_stats = defaultdict(lambda: {'count': 0, 'size': 0})
        
    def parse(self):
        """è§£æHPROFæ–‡ä»¶"""
        try:
            self.file = open(self.filename, 'rb')
            self._parse_header()
            self._parse_records()
            self._analyze_memory()
            return True
        except Exception as e:
            print(f"è§£æHPROFæ–‡ä»¶å¤±è´¥: {e}")
            return False
        finally:
            if self.file:
                self.file.close()
    
    def _parse_header(self):
        """è§£ææ–‡ä»¶å¤´"""
        # è¯»å–ç‰ˆæœ¬å­—ç¬¦ä¸² (ä»¥nullç»“å°¾)
        version_bytes = []
        while True:
            byte = self.file.read(1)
            if not byte or byte == b'\x00':
                break
            version_bytes.append(byte)
        
        self.header['version'] = b''.join(version_bytes).decode('ascii')
        
        # è¯»å–æ ‡è¯†ç¬¦å¤§å°ï¼ˆ4å­—èŠ‚ï¼‰
        id_size_bytes = self.file.read(4)
        self.header['id_size'] = struct.unpack('>I', id_size_bytes)[0]
        
        # è¯»å–æ—¶é—´æˆ³ï¼ˆ8å­—èŠ‚ï¼‰
        timestamp_bytes = self.file.read(8)
        self.header['timestamp'] = struct.unpack('>Q', timestamp_bytes)[0]
        
        print(f"HPROFç‰ˆæœ¬: {self.header['version']}")
        print(f"æ ‡è¯†ç¬¦å¤§å°: {self.header['id_size']} bytes")
        print(f"æ—¶é—´æˆ³: {datetime.fromtimestamp(self.header['timestamp']/1000)}")
    
    def _read_id(self):
        """è¯»å–ID"""
        id_bytes = self.file.read(self.header['id_size'])
        if len(id_bytes) != self.header['id_size']:
            return None
        if self.header['id_size'] == 4:
            return struct.unpack('>I', id_bytes)[0]
        elif self.header['id_size'] == 8:
            return struct.unpack('>Q', id_bytes)[0]
        else:
            return int.from_bytes(id_bytes, 'big')
    
    def _parse_records(self):
        """è§£æè®°å½•"""
        while True:
            # è¯»å–è®°å½•å¤´
            record_header = self.file.read(9)  # 1å­—èŠ‚ç±»å‹ + 4å­—èŠ‚æ—¶é—´æˆ³ + 4å­—èŠ‚é•¿åº¦
            if len(record_header) != 9:
                break
                
            record_type = record_header[0]
            timestamp = struct.unpack('>I', record_header[1:5])[0]
            length = struct.unpack('>I', record_header[5:9])[0]
            
            # æ ¹æ®è®°å½•ç±»å‹å¤„ç†
            if record_type == HprofRecord.UTF8:
                self._parse_utf8_record(length)
            elif record_type == HprofRecord.LOAD_CLASS:
                self._parse_load_class_record(length)
            elif record_type in [HprofRecord.HEAP_DUMP, HprofRecord.HEAP_DUMP_SEGMENT]:
                self._parse_heap_dump_record(length)
            else:
                # è·³è¿‡å…¶ä»–ç±»å‹çš„è®°å½•
                self.file.seek(length, 1)
    
    def _parse_utf8_record(self, length):
        """è§£æUTF8å­—ç¬¦ä¸²è®°å½•"""
        string_id = self._read_id()
        string_data = self.file.read(length - self.header['id_size'])
        try:
            string_value = string_data.decode('utf-8')
            self.strings[string_id] = string_value
        except UnicodeDecodeError:
            self.strings[string_id] = str(string_data)
    
    def _parse_load_class_record(self, length):
        """è§£æç±»åŠ è½½è®°å½•"""
        class_serial = struct.unpack('>I', self.file.read(4))[0]
        class_id = self._read_id()
        stack_trace_serial = struct.unpack('>I', self.file.read(4))[0]
        class_name_id = self._read_id()
        
        self.classes[class_id] = {
            'serial': class_serial,
            'name_id': class_name_id,
            'name': self.strings.get(class_name_id, f"Unknown_{class_name_id}"),
            'stack_trace': stack_trace_serial
        }
    
    def _parse_heap_dump_record(self, length):
        """è§£æå †dumpè®°å½•"""
        end_pos = self.file.tell() + length
        
        while self.file.tell() < end_pos:
            sub_record_type = self.file.read(1)
            if not sub_record_type:
                break
            sub_record_type = sub_record_type[0]
            
            if sub_record_type == HprofHeapDump.CLASS_DUMP:
                self._parse_class_dump()
            elif sub_record_type == HprofHeapDump.INSTANCE_DUMP:
                self._parse_instance_dump()
            elif sub_record_type == HprofHeapDump.OBJECT_ARRAY_DUMP:
                self._parse_object_array_dump()
            elif sub_record_type == HprofHeapDump.PRIMITIVE_ARRAY_DUMP:
                self._parse_primitive_array_dump()
            elif sub_record_type in [HprofHeapDump.ROOT_JNI_GLOBAL, HprofHeapDump.ROOT_JNI_LOCAL,
                                   HprofHeapDump.ROOT_JAVA_FRAME, HprofHeapDump.ROOT_NATIVE_STACK,
                                   HprofHeapDump.ROOT_STICKY_CLASS, HprofHeapDump.ROOT_THREAD_BLOCK,
                                   HprofHeapDump.ROOT_MONITOR_USED, HprofHeapDump.ROOT_THREAD_OBJECT]:
                self._parse_root_record(sub_record_type)
            else:
                # è·³è¿‡æœªçŸ¥çš„å­è®°å½•ç±»å‹
                break
    
    def _parse_class_dump(self):
        """è§£æç±»dump"""
        class_id = self._read_id()
        stack_trace_serial = struct.unpack('>I', self.file.read(4))[0]
        super_class_id = self._read_id()
        class_loader_id = self._read_id()
        signers_id = self._read_id()
        protection_domain_id = self._read_id()
        reserved1 = self._read_id()
        reserved2 = self._read_id()
        instance_size = struct.unpack('>I', self.file.read(4))[0]
        
        # è¯»å–å¸¸é‡æ± 
        constant_pool_size = struct.unpack('>H', self.file.read(2))[0]
        for _ in range(constant_pool_size):
            index = struct.unpack('>H', self.file.read(2))[0]
            type_byte = self.file.read(1)[0]
            self._skip_value_by_type(type_byte)
        
        # è¯»å–é™æ€å­—æ®µ
        static_fields_count = struct.unpack('>H', self.file.read(2))[0]
        for _ in range(static_fields_count):
            name_id = self._read_id()
            type_byte = self.file.read(1)[0]
            self._skip_value_by_type(type_byte)
        
        # è¯»å–å®ä¾‹å­—æ®µ
        instance_fields_count = struct.unpack('>H', self.file.read(2))[0]
        for _ in range(instance_fields_count):
            name_id = self._read_id()
            type_byte = self.file.read(1)[0]
        
        # æ›´æ–°ç±»ä¿¡æ¯
        if class_id in self.classes:
            self.classes[class_id]['instance_size'] = instance_size
    
    def _parse_instance_dump(self):
        """è§£æå®ä¾‹dump"""
        object_id = self._read_id()
        stack_trace_serial = struct.unpack('>I', self.file.read(4))[0]
        class_id = self._read_id()
        instance_data_length = struct.unpack('>I', self.file.read(4))[0]
        
        # è·³è¿‡å®ä¾‹æ•°æ®
        self.file.seek(instance_data_length, 1)
        
        # ç»Ÿè®¡
        class_name = "Unknown"
        if class_id in self.classes:
            class_name = self.classes[class_id]['name']
        
        self.instances[object_id] = {
            'class_id': class_id,
            'class_name': class_name,
            'size': instance_data_length,
            'stack_trace': stack_trace_serial
        }
        
        self.total_instances += 1
        self.total_instance_size += instance_data_length
        self.class_stats[class_name]['count'] += 1
        self.class_stats[class_name]['size'] += instance_data_length
        
        # ç‰¹æ®Šå¤„ç†Stringå¯¹è±¡
        if 'String' in class_name:
            self.string_stats['count'] += 1
            self.string_stats['size'] += instance_data_length
    
    def _parse_object_array_dump(self):
        """è§£æå¯¹è±¡æ•°ç»„dump"""
        array_id = self._read_id()
        stack_trace_serial = struct.unpack('>I', self.file.read(4))[0]
        array_length = struct.unpack('>I', self.file.read(4))[0]
        element_class_id = self._read_id()
        
        # è·³è¿‡æ•°ç»„å…ƒç´ 
        element_size = self.header['id_size']
        total_size = array_length * element_size
        self.file.seek(total_size, 1)
        
        # è·å–å…ƒç´ ç±»å
        element_class_name = "Unknown"
        if element_class_id in self.classes:
            element_class_name = self.classes[element_class_id]['name']
        
        array_name = f"{element_class_name}[]"
        
        self.arrays[array_id] = {
            'element_class_id': element_class_id,
            'element_class_name': element_class_name,
            'length': array_length,
            'size': total_size,
            'type': 'object'
        }
        
        self.total_arrays += 1
        self.total_array_size += total_size
        self.class_stats[array_name]['count'] += 1
        self.class_stats[array_name]['size'] += total_size
    
    def _parse_primitive_array_dump(self):
        """è§£æåŸºæœ¬ç±»å‹æ•°ç»„dump"""
        array_id = self._read_id()
        stack_trace_serial = struct.unpack('>I', self.file.read(4))[0]
        array_length = struct.unpack('>I', self.file.read(4))[0]
        element_type = self.file.read(1)[0]
        
        # åŸºæœ¬ç±»å‹å¤§å°æ˜ å°„
        type_sizes = {
            4: 1,   # boolean
            5: 2,   # char
            6: 4,   # float
            7: 8,   # double
            8: 1,   # byte
            9: 2,   # short
            10: 4,  # int
            11: 8   # long
        }
        
        type_names = {
            4: 'boolean',
            5: 'char',
            6: 'float',
            7: 'double',
            8: 'byte',
            9: 'short',
            10: 'int',
            11: 'long'
        }
        
        element_size = type_sizes.get(element_type, 1)
        total_size = array_length * element_size
        type_name = type_names.get(element_type, f'type_{element_type}')
        array_name = f"{type_name}[]"
        
        # è·³è¿‡æ•°ç»„æ•°æ®
        self.file.seek(total_size, 1)
        
        self.arrays[array_id] = {
            'element_type': element_type,
            'element_type_name': type_name,
            'length': array_length,
            'size': total_size,
            'type': 'primitive'
        }
        
        self.total_arrays += 1
        self.total_array_size += total_size
        self.primitive_stats[array_name]['count'] += 1
        self.primitive_stats[array_name]['size'] += total_size
    
    def _parse_root_record(self, root_type):
        """è§£ææ ¹å¯¹è±¡è®°å½•"""
        object_id = self._read_id()
        
        root_info = {'type': root_type, 'object_id': object_id}
        
        # æ ¹æ®æ ¹ç±»å‹è¯»å–é¢å¤–ä¿¡æ¯
        if root_type in [HprofHeapDump.ROOT_JNI_LOCAL, HprofHeapDump.ROOT_JAVA_FRAME]:
            thread_serial = struct.unpack('>I', self.file.read(4))[0]
            frame_number = struct.unpack('>I', self.file.read(4))[0]
            root_info['thread_serial'] = thread_serial
            root_info['frame_number'] = frame_number
        elif root_type == HprofHeapDump.ROOT_THREAD_OBJECT:
            thread_serial = struct.unpack('>I', self.file.read(4))[0]
            stack_trace_serial = struct.unpack('>I', self.file.read(4))[0]
            root_info['thread_serial'] = thread_serial
            root_info['stack_trace_serial'] = stack_trace_serial
        
        self.roots.append(root_info)
    
    def _skip_value_by_type(self, type_byte):
        """æ ¹æ®ç±»å‹è·³è¿‡å€¼"""
        type_sizes = {
            2: self.header['id_size'],  # object
            4: 1,   # boolean
            5: 2,   # char
            6: 4,   # float
            7: 8,   # double
            8: 1,   # byte
            9: 2,   # short
            10: 4,  # int
            11: 8   # long
        }
        size = type_sizes.get(type_byte, 0)
        if size > 0:
            self.file.seek(size, 1)
    
    def _analyze_memory(self):
        """åˆ†æå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        print("\n=== å†…å­˜åˆ†æå®Œæˆ ===")
        print(f"æ€»å®ä¾‹æ•°: {self.total_instances:,}")
        print(f"å®ä¾‹æ€»å¤§å°: {self.total_instance_size / 1024 / 1024:.2f} MB")
        print(f"æ€»æ•°ç»„æ•°: {self.total_arrays:,}")
        print(f"æ•°ç»„æ€»å¤§å°: {self.total_array_size / 1024 / 1024:.2f} MB")
        print(f"æ€»å†…å­˜ä½¿ç”¨: {(self.total_instance_size + self.total_array_size) / 1024 / 1024:.2f} MB")
    
    def print_class_statistics(self, top_n=20, min_size_mb=0.1):
        """æ‰“å°ç±»ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n=== TOP {top_n} å†…å­˜å ç”¨ç±» (æœ€å° {min_size_mb}MB) ===")
        print(f"{'ç±»å':<50} {'å®ä¾‹æ•°':<10} {'æ€»å¤§å°(MB)':<12} {'å¹³å‡å¤§å°(KB)':<12}")
        print("-" * 90)
        
        # æŒ‰æ€»å¤§å°æ’åº
        sorted_classes = sorted(self.class_stats.items(), 
                              key=lambda x: x[1]['size'], reverse=True)
        
        count = 0
        for class_name, stats in sorted_classes:
            size_mb = stats['size'] / 1024 / 1024
            if size_mb < min_size_mb:
                continue
                
            avg_size_kb = stats['size'] / stats['count'] / 1024 if stats['count'] > 0 else 0
            print(f"{class_name:<50} {stats['count']:<10,} {size_mb:<12.2f} {avg_size_kb:<12.2f}")
            
            count += 1
            if count >= top_n:
                break
    
    def print_primitive_statistics(self, top_n=10):
        """æ‰“å°åŸºæœ¬ç±»å‹æ•°ç»„ç»Ÿè®¡"""
        print(f"\n=== TOP {top_n} åŸºæœ¬ç±»å‹æ•°ç»„å†…å­˜å ç”¨ ===")
        print(f"{'æ•°ç»„ç±»å‹':<20} {'æ•°ç»„æ•°é‡':<10} {'æ€»å¤§å°(MB)':<12} {'å¹³å‡å¤§å°(KB)':<12}")
        print("-" * 60)
        
        sorted_primitives = sorted(self.primitive_stats.items(), 
                                 key=lambda x: x[1]['size'], reverse=True)
        
        for i, (array_type, stats) in enumerate(sorted_primitives[:top_n]):
            size_mb = stats['size'] / 1024 / 1024
            avg_size_kb = stats['size'] / stats['count'] / 1024 if stats['count'] > 0 else 0
            print(f"{array_type:<20} {stats['count']:<10,} {size_mb:<12.2f} {avg_size_kb:<12.2f}")
    
    def print_string_statistics(self):
        """æ‰“å°å­—ç¬¦ä¸²ç»Ÿè®¡"""
        print(f"\n=== å­—ç¬¦ä¸²å†…å­˜ç»Ÿè®¡ ===")
        print(f"å­—ç¬¦ä¸²å®ä¾‹æ•°: {self.string_stats['count']:,}")
        print(f"å­—ç¬¦ä¸²æ€»å¤§å°: {self.string_stats['size'] / 1024 / 1024:.2f} MB")
        if self.string_stats['count'] > 0:
            avg_size = self.string_stats['size'] / self.string_stats['count']
            print(f"å¹³å‡å­—ç¬¦ä¸²å¤§å°: {avg_size:.2f} bytes")
    
    def export_analysis(self, output_file):
        """å¯¼å‡ºåˆ†æç»“æœ"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("Android HPROFå†…å­˜åˆ†ææŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            f.write(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"HPROFæ–‡ä»¶: {self.filename}\n\n")
            
            # æ€»ä½“ç»Ÿè®¡
            f.write("æ€»ä½“å†…å­˜ç»Ÿè®¡:\n")
            f.write("-" * 30 + "\n")
            f.write(f"æ€»å®ä¾‹æ•°: {self.total_instances:,}\n")
            f.write(f"å®ä¾‹æ€»å¤§å°: {self.total_instance_size / 1024 / 1024:.2f} MB\n")
            f.write(f"æ€»æ•°ç»„æ•°: {self.total_arrays:,}\n")
            f.write(f"æ•°ç»„æ€»å¤§å°: {self.total_array_size / 1024 / 1024:.2f} MB\n")
            f.write(f"æ€»å†…å­˜ä½¿ç”¨: {(self.total_instance_size + self.total_array_size) / 1024 / 1024:.2f} MB\n\n")
            
            # ç±»ç»Ÿè®¡
            f.write("TOP 50 å†…å­˜å ç”¨ç±»:\n")
            f.write("-" * 30 + "\n")
            f.write(f"{'ç±»å':<60} {'å®ä¾‹æ•°':<12} {'æ€»å¤§å°(MB)':<15} {'å¹³å‡å¤§å°(KB)':<15}\n")
            f.write("-" * 110 + "\n")
            
            sorted_classes = sorted(self.class_stats.items(), 
                                  key=lambda x: x[1]['size'], reverse=True)
            
            for class_name, stats in sorted_classes[:50]:
                size_mb = stats['size'] / 1024 / 1024
                avg_size_kb = stats['size'] / stats['count'] / 1024 if stats['count'] > 0 else 0
                f.write(f"{class_name:<60} {stats['count']:<12,} {size_mb:<15.2f} {avg_size_kb:<15.2f}\n")
            
            # åŸºæœ¬ç±»å‹æ•°ç»„ç»Ÿè®¡
            f.write(f"\nåŸºæœ¬ç±»å‹æ•°ç»„ç»Ÿè®¡:\n")
            f.write("-" * 30 + "\n")
            f.write(f"{'æ•°ç»„ç±»å‹':<20} {'æ•°ç»„æ•°é‡':<12} {'æ€»å¤§å°(MB)':<15} {'å¹³å‡å¤§å°(KB)':<15}\n")
            f.write("-" * 70 + "\n")
            
            sorted_primitives = sorted(self.primitive_stats.items(), 
                                     key=lambda x: x[1]['size'], reverse=True)
            
            for array_type, stats in sorted_primitives:
                size_mb = stats['size'] / 1024 / 1024
                avg_size_kb = stats['size'] / stats['count'] / 1024 if stats['count'] > 0 else 0
                f.write(f"{array_type:<20} {stats['count']:<12,} {size_mb:<15.2f} {avg_size_kb:<15.2f}\n")
            
            # å­—ç¬¦ä¸²ç»Ÿè®¡
            f.write(f"\nå­—ç¬¦ä¸²ç»Ÿè®¡:\n")
            f.write("-" * 30 + "\n")
            f.write(f"å­—ç¬¦ä¸²å®ä¾‹æ•°: {self.string_stats['count']:,}\n")
            f.write(f"å­—ç¬¦ä¸²æ€»å¤§å°: {self.string_stats['size'] / 1024 / 1024:.2f} MB\n")
            if self.string_stats['count'] > 0:
                avg_size = self.string_stats['size'] / self.string_stats['count']
                f.write(f"å¹³å‡å­—ç¬¦ä¸²å¤§å°: {avg_size:.2f} bytes\n")
            
            # Educational Resources Section
            f.write("\n" + "="*60 + "\n")
            f.write("ğŸ“š æ·±å…¥å­¦ä¹ æŒ‡å— / Educational Resources\n")
            f.write("="*60 + "\n\n")
            f.write("ä¸ºäº†æ›´å¥½åœ°ç†è§£ HPROF åˆ†æç»“æœï¼Œå»ºè®®é˜…è¯»ä»¥ä¸‹è¯¦ç»†æŒ‡å—ï¼š\n")
            f.write("For better understanding of HPROF analysis results, please refer to these detailed guides:\n\n")
            
            f.write("ğŸ” åŸºç¡€å†…å­˜åˆ†æ / Basic Memory Analysis:\n")
            f.write("  â€¢ dumpsys meminfo è¾“å‡ºè¯¦è§£æŒ‡å— / dumpsys meminfo Interpretation Guide:\n")
            f.write("    ./meminfo_interpretation_guide.md\n")
            f.write("    åº”ç”¨çº§å†…å­˜ä½¿ç”¨åˆ†æï¼Œç†è§£åº”ç”¨å†…å­˜åˆ†å¸ƒå’Œä½¿ç”¨çŠ¶å†µ\n\n")
            
            f.write("  â€¢ /proc/meminfo è¾“å‡ºè¯¦è§£æŒ‡å— / /proc/meminfo Interpretation Guide:\n")
            f.write("    ./proc_meminfo_interpretation_guide.md\n")
            f.write("    ç³»ç»Ÿçº§å†…å­˜ä½¿ç”¨åˆ†æï¼Œç†è§£è®¾å¤‡æ•´ä½“å†…å­˜çŠ¶å†µ\n\n")
            
            f.write("  â€¢ showmap è¾“å‡ºè¯¦è§£æŒ‡å— / showmap Interpretation Guide:\n")
            f.write("    ./showmap_interpretation_guide.md\n")
            f.write("    è¿›ç¨‹çº§å†…å­˜æ˜ å°„æ¦‚è§ˆï¼Œå¿«é€Ÿè¯†åˆ«å†…å­˜ä½¿ç”¨æ¨¡å¼\n\n")
            
            f.write("ğŸ—ºï¸ è¯¦ç»†å†…å­˜åˆ†æ / Detailed Memory Analysis:\n")
            f.write("  â€¢ smaps è¾“å‡ºè¯¦è§£æŒ‡å— / smaps Interpretation Guide:\n")
            f.write("    ./smaps_interpretation_guide.md\n")
            f.write("    æœ€è¯¦ç»†çš„å†…å­˜æ˜ å°„åˆ†æï¼Œæ·±å…¥ç†è§£æ¯ä¸ªå†…å­˜åŒºåŸŸ\n\n")
            
            f.write("ğŸ“Š è§£æç»“æœç†è§£ / Understanding Analysis Results:\n")
            f.write("  â€¢ åˆ†æç»“æœè¯¦è§£æŒ‡å— / Analysis Results Interpretation Guide:\n")
            f.write("    ./analysis_results_interpretation_guide.md\n")
            f.write("    ç†è§£æœ¬å·¥å…·è¾“å‡ºçš„æ¯ä¸€é¡¹æ•°æ®å’Œä¼˜åŒ–å»ºè®®\n\n")
            
            f.write("â˜• Java å †å†…å­˜ä¸“é¡¹ä¼˜åŒ– / Java Heap Memory Optimization:\n")
            f.write("   â€¢ ç»“åˆ SMAPS åˆ†æäº†è§£å®Œæ•´å†…å­˜ä½¿ç”¨æƒ…å†µ\n")
            f.write("   â€¢ ä½¿ç”¨ MAT (Memory Analyzer Tool) è¿›è¡Œæ·±åº¦å¼•ç”¨é“¾åˆ†æ\n")
            f.write("   â€¢ é…åˆ LeakCanary è‡ªåŠ¨æ£€æµ‹å†…å­˜æ³„æ¼\n")
            f.write("   â€¢ å…³æ³¨å¤§å¯¹è±¡ã€é‡å¤å­—ç¬¦ä¸²å’Œé›†åˆç±»çš„ä½¿ç”¨\n\n")

def main():
    parser = argparse.ArgumentParser(description="Android HPROFå†…å­˜åˆ†æå·¥å…·")
    parser.add_argument('-f', '--file', required=True, help="HPROFæ–‡ä»¶è·¯å¾„")
    parser.add_argument('-o', '--output', help="åˆ†æç»“æœè¾“å‡ºæ–‡ä»¶")
    parser.add_argument('-t', '--top', type=int, default=20, help="æ˜¾ç¤ºTOP Nä¸ªå†…å­˜å ç”¨ç±» (é»˜è®¤20)")
    parser.add_argument('-m', '--min-size', type=float, default=0.1, help="æœ€å°æ˜¾ç¤ºå¤§å°(MB) (é»˜è®¤0.1)")
    parser.add_argument('-s', '--simple', action='store_true', help="ç®€å•è¾“å‡ºæ¨¡å¼")
    
    args = parser.parse_args()
    
    if not os.path.exists(args.file):
        print(f"é”™è¯¯: HPROFæ–‡ä»¶ä¸å­˜åœ¨: {args.file}")
        return
    
    print(f"å¼€å§‹è§£æHPROFæ–‡ä»¶: {args.file}")
    
    parser = HprofParser(args.file)
    if parser.parse():
        if not args.simple:
            parser.print_class_statistics(args.top, args.min_size)
            parser.print_primitive_statistics()
            parser.print_string_statistics()
        
        if args.output:
            parser.export_analysis(args.output)
            print(f"\nåˆ†æç»“æœå·²å¯¼å‡ºåˆ°: {args.output}")
        else:
            # é»˜è®¤è¾“å‡ºæ–‡ä»¶
            base_name = os.path.splitext(os.path.basename(args.file))[0]
            default_output = f"{base_name}_analysis.txt"
            parser.export_analysis(default_output)
            print(f"\nåˆ†æç»“æœå·²å¯¼å‡ºåˆ°: {default_output}")
        
        # Educational Resources for Console Output
        print("\n" + "="*60)
        print("ğŸ“š æ·±å…¥å­¦ä¹ æŒ‡å— / Educational Resources")
        print("="*60)
        print("\nä¸ºäº†æ›´å¥½åœ°ç†è§£ HPROF åˆ†æç»“æœï¼Œå»ºè®®é˜…è¯»ä»¥ä¸‹è¯¦ç»†æŒ‡å—ï¼š")
        print("For better understanding of HPROF analysis results, please refer to these detailed guides:\n")
        
        print("ğŸ” åŸºç¡€å†…å­˜åˆ†æ / Basic Memory Analysis:")
        print("  â€¢ dumpsys meminfo è¾“å‡ºè¯¦è§£æŒ‡å— / dumpsys meminfo Interpretation Guide:")
        print("    ./meminfo_interpretation_guide.md")
        print("    åº”ç”¨çº§å†…å­˜ä½¿ç”¨åˆ†æï¼Œç†è§£åº”ç”¨å†…å­˜åˆ†å¸ƒå’Œä½¿ç”¨çŠ¶å†µ\n")
        
        print("  â€¢ /proc/meminfo è¾“å‡ºè¯¦è§£æŒ‡å— / /proc/meminfo Interpretation Guide:")
        print("    ./proc_meminfo_interpretation_guide.md")
        print("    ç³»ç»Ÿçº§å†…å­˜ä½¿ç”¨åˆ†æï¼Œç†è§£è®¾å¤‡æ•´ä½“å†…å­˜çŠ¶å†µ\n")
        
        print("  â€¢ showmap è¾“å‡ºè¯¦è§£æŒ‡å— / showmap Interpretation Guide:")
        print("    ./showmap_interpretation_guide.md")
        print("    è¿›ç¨‹çº§å†…å­˜æ˜ å°„æ¦‚è§ˆï¼Œå¿«é€Ÿè¯†åˆ«å†…å­˜ä½¿ç”¨æ¨¡å¼\n")
        
        print("ğŸ—ºï¸ è¯¦ç»†å†…å­˜åˆ†æ / Detailed Memory Analysis:")
        print("  â€¢ smaps è¾“å‡ºè¯¦è§£æŒ‡å— / smaps Interpretation Guide:")
        print("    ./smaps_interpretation_guide.md")
        print("    æœ€è¯¦ç»†çš„å†…å­˜æ˜ å°„åˆ†æï¼Œæ·±å…¥ç†è§£æ¯ä¸ªå†…å­˜åŒºåŸŸ\n")
        
        print("ğŸ“Š è§£æç»“æœç†è§£ / Understanding Analysis Results:")
        print("  â€¢ åˆ†æç»“æœè¯¦è§£æŒ‡å— / Analysis Results Interpretation Guide:")
        print("    ./analysis_results_interpretation_guide.md")
        print("    ç†è§£æœ¬å·¥å…·è¾“å‡ºçš„æ¯ä¸€é¡¹æ•°æ®å’Œä¼˜åŒ–å»ºè®®\n")
        
        print("â˜• Java å †å†…å­˜ä¸“é¡¹ä¼˜åŒ– / Java Heap Memory Optimization:")
        print("   â€¢ ç»“åˆ SMAPS åˆ†æäº†è§£å®Œæ•´å†…å­˜ä½¿ç”¨æƒ…å†µ")
        print("   â€¢ ä½¿ç”¨ MAT (Memory Analyzer Tool) è¿›è¡Œæ·±åº¦å¼•ç”¨é“¾åˆ†æ")
        print("   â€¢ é…åˆ LeakCanary è‡ªåŠ¨æ£€æµ‹å†…å­˜æ³„æ¼")
        print("   â€¢ å…³æ³¨å¤§å¯¹è±¡ã€é‡å¤å­—ç¬¦ä¸²å’Œé›†åˆç±»çš„ä½¿ç”¨\n")
    else:
        print("HPROFæ–‡ä»¶è§£æå¤±è´¥")

if __name__ == "__main__":
    main()